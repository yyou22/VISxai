[meta title:"A Beginner's Introduction to Adversarial Attacks by Yuzhe You and Jian Zhao" description:"A Beginner's Introduction to Adversarial Attacks" /]

[var name:"perturbation_val" value:0 /]

[var name:"scrollState" value:"loading" /]
[Fixed]
  [DRComponent
    state:scrollState
    perturb:perturbation_val
  /]
[/Fixed]

//scroller
[Scroller currentState:scrollState]

  [Graphic]
    [ul className:"circles__"]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
    [/ul]
    [div className:"background"]
    [/div]
  [/Graphic]

  [Step state:"abstract"]

    [CustomHeader
        title:"Panda or Gibbon? How CNNs Get Fooled by Input Noises"
        subtitle:"A Beginner's Introduction to Adversarial Attacks"
        date:"July 30, 2024"
        authors:`[
        { name: "Yuzhe You", link: "https://yuzhe.vercel.app/" },
        { name: "Jian Zhao", link: "https://www.jeffjianzhao.com/" }
        ]` /]

    [div className:"line"][/div]

    [div className:"text"]
        ## ABSTRACT
        Though deep learning models have achieved remarkable success in diverse domains (e.g., facial recognition, autonomous driving), these models have been proven to be quite brittle to perturbations around the input data. 
        [span className:"emphasize"]Adversarial machine learning (AML)[/span] studies attacks that can fool machine learning models into generating incorrect outcomes as well as the defenses against worst-case attacks to strengthen model robustness. 
        Specifically, for image classification, it is challenging to understand [span className:"emphasize"]adversarial attacks[/span] due to their use of subtle perturbations that are not human-interpretable, as well as the variability of attack impacts influenced by attack methods, instance differences, or model architectures. 
        This guide will utilize interactive visualizations to provide a non-expert introduction to adversarial attacks, and visualize the impact of FGSM attacks on two different ResNet-34 models. 
        We designed this guide for beginners who are familiar with basic machine learning but new to advanced AML topics like adversarial attacks.
 
    [/div]
  [/Step]

  [Step state:"beginning"]
    [div className:"text"]
      ## FROM THE BEGINNING
      Before we use visualizations to understand the impacts of adversarial attacks on CNNs (yes, those are the soon-to-be data points you see floating on the right), let's first go through the basics of adversarial attacks.
    [/div]
    [div className:"text"]
      ### What is an Adversarial Attack?
      In 2014, Goodfellow et al. [cite reference:"goodfellow2014explaining"/] showed that an adversarial image of a [span className:"emphasize"]panda[/span] could fool GoogLeNet into classifying it as a [span className:"emphasize"]gibbon[/span] with high confidence, leading to the birth of AML research. An adversarial [span className:"emphasize"]"evasion" attack[/span] produces adversarial examples that are crafted with small, indistinguishable perturbations with the goal of causing model prediction errors, such as image misclassifications. 
    [/div]
    [figure]
      [img src:"static/images/tut1.png" className:"img_" /]
    [/figure]
    [div className:"text"]
      ### What is an FGSM Attack?
      The panda attack you just saw is called the [span className:"emphasize"]Fast Gradient Sign Method (FGSM)[/span] attack, one of the first and most well-known adversarial attacks. It is a [span className:"emphasize"]white-box attack[/span] (i.e., the attacker has access to model internals) that adjusts the input image by taking a step in the direction of the sign of the backpropagated gradients.
    [/div]
      [figure]
      [img src:"static/images/tut2.png" className:"img_" /]
      [/figure]

    [var name:"showFGSM" value:false /]

    [conditional if:`!showFGSM`]
      [button onClick:`showFGSM = true;`]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:`showFGSM`]

      [div className:"text"]
        //The idea is that rather than working to minimize the loss like in model training, the FGSM attack adjusts the input data to maximize the loss:
        The idea is to manipulate the input data to maximize the loss, instead of minimizing it like we do during model training.
        In other words, it makes small changes to the input image to push the model to make a wrong prediction:
        [br/]
        [Equation display:true]
        \mathbf{x}' = \mathbf{x} +\epsilon \textrm{\textrm{sign}}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y)). 
        [/Equation]
      [/div]
      [div className:"panel"]
        Specifically,
        [br/]
* [Equation]\mathbf{x}'[/Equation]: This is the adversarial image.
* [Equation]\mathbf{x}[/Equation]: This is the original input image.
* [Equation]J(\theta, \mathbf{x}, y)[/Equation]: This is the loss function that measures how far the model's predictions are from the true label [Equation]y[/Equation]. It depends on the model's parameters [Equation]\theta[/Equation], the input image [Equation]\mathbf{x}[/Equation], and the true label [Equation]y[/Equation].
* [Equation]\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y)[/Equation]: This is the gradient of the loss function w.r.t the input image. It tells the attacker in which direction the input should be changed to increase the loss.
* [Equation]\textrm{sign}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y))[/Equation]: This extracts the direction (+ or -) of the gradient, indicating whether each pixel should be increased or decreased to result in model error.
* [Equation]\epsilon[/Equation]: This controls the magnitude of the perturbation. If [Equation]\epsilon[/Equation] is too large, the changes become too noticeable to humans; if too small, the attack might not be effective.
      [/div]
      [div className:"text"]
        In short, the FGSM attack creates an [span className:"emphasize"]adversarial image[/span] by adding a small amount of perturbation to the original image in the direction that increases the model's loss. 
        This [span className:"emphasize"]perturbation[/span] [Equation]\epsilon[/Equation] is enough to fool machine learning models, but usually so subtle that a human wouldn’t really notice them.
      [/div]
      [figure]
        [img src:"static/images/tut3.png" className:"img_2" / ]
      [/figure]

      [button onClick:`showFGSM = false;`]
        Hide technical details
      [/button]

    [/conditional]

    [div className:"text"]
        ### How does the CNN see a gibbon then?
        While the attack logic is simple and intuitive, how can two panda images that appear indistinguishable to humans seem so different to machine learning models? 
        [br/]
        [br/]
        [span className:"emphasize2"]
        Let's examine how the FGSM attack alters a CNN's perception of image datasets.
        [/span]
      [/div]

  [/Step]

  [Step state:"dataset"]
    [div className:"text"]
      ## DATASET & MODEL
      ### CIFAR-10 Dataset
      On the right, we have loaded 100 sampled images from the [span className:"emphasize"]CIFAR-10 dataset[/span] [cite reference:"krizhevsky2009learning"/] in the scatterplot. The CIFAR-10 dataset consists of 60,000 (32 × 32) colored images from [span className:"emphasize"]10 different classes[/span] (50,000 training data and 10,000 testing data), with 6,000 images per class. Here, we randomly sampled 10 images from each class.
      [br/]
      [br/]
      We use the following colors to represent these classes:
    [/div]
    [figure]
      [img src:"static/images/color legend.png" className:"img_" / ]
    [/figure]
    [div className:"text"]
      Each circle in the scatterplot represents an instance from the dataset and is split into two halves: the color of the left half represents its [span className:"emphasize"]ground truth label[/span], while the color of the right half represents the [span className:"emphasize"]model's prediction[/span] of the image.
    [/div]
    [figure]
      [img src:"static/images/circle_.png" className:"img_3" / ]
    [/figure]
    [div className:"text"]
      ### ResNet-34 Model
      For this article, we will use the [span className:"emphasize"]ResNet-34[/span] model and start by visualizing its perception of the CIFAR-10 dataset by extracting its embeddings.
      [br/]
      [var name:"showResNet" value:false /]

      [conditional if:`!showResNet`]
        [button onClick:`showResNet = true;`]
          Show more about ResNet-34
        [/button]
      [/conditional]

      [conditional if:`showResNet`]
        ResNet-34[cite reference:"he2016deep"/] is a [span className:"emphasize"]convolutional neural network (CNN)[/span] that is part of the Residual Networks family, which utilizes [span className:"emphasize"]residual learning[/span]. 
        In traditional DNNs, each layer learns a full transformation from the input to the desired output.  
        However, as networks become deeper, training becomes more challenging due to problems like the *vanishing gradient*.
        [br/]
        [br/]
        Residual learning addresses this by focusing on the *residual* — the difference between the desired layer output and what earlier layers produced. 
        This way, each layer only learns what to add to the previous output. 
        This is done through *skip connections* that pass one layer’s output directly to later layers.
        The architecture we selected consists of 34 layers and is widely adopted for its strong performance in image classification.
        [br/]
        [br/]
        [button onClick:`showResNet = false;`]
          Show less about ResNet-34
        [/button]

      [/conditional]

      [br/]
      To extract ResNet-34's perception of the images, we temporarily detach the final output layer to obtain the [span className:"emphasize"]embeddings[/span], which are high-dimensional representations capturing the essential features of the input images.
    [/div]
    [div className:"text"]
      ### Exploring the Data Points
      [span className:"emphasize2"]
      Start exploring the data points by hovering over them and observing their ground truth labels and ResNet-34's predictions.
      [/span]
    [/div]
  [/Step]

  [Step state:"data points"]
    [div className:"text"]
      ## INTERACTING WITH DATA POINTS
      ### Projecting onto a 2-D Space
      To reveal important patterns in the embeddings and transform them into a format that can be easily visualized, we apply [span className:"emphasize"]dimensionality reduction[/span] to project the model embeddings into a lower dimension. We start by using [span className:"emphasize"]t-SNE[/span] (t-distributed Stochastic Neighbor Embedding) [cite reference:"van2008visualizing"/].
      [br/]
      [br/]
      t-SNE works by converting similarities between data points into probabilities and then maps these points into a 2-D space while preserving the relative distances. It highlights clusters and relationships in the data that aren't easily seen in higher dimensions.
      [br/]
      [br/]
      The resulting outputs are scaled to be used as the x- and y-coordinates of the instances in the scatterplot shown on the right.
      [br/]
      [br/]
      [span className:"emphasize2"]
        Note: although t-SNE is a powerful tool and often produces visually impressive results [cite reference:"espadoto2019toward"/][cite reference:"atzberger2023large"/], it does come with certain limitations and must be used cautiously.
      [/span]
    [/div]

    [var name:"showLimitation" value:false /]

    [conditional if:`!showLimitation`]
      [button onClick:`showLimitation = true;`]
        Show t-SNE's Limitations
      [/button]
    [/conditional]

    [conditional if:`showLimitation`]

      [div className:"panel"]
* **Sensitivity to initialization** [cite reference:"kobak2021initialization"/]: t-SNE's produced results can vary based on its initial conditions, which means that different runs could potentially produce different results. In our case, we average the results from multiple runs to create a more stable and reliable representation.
* **Local linearity assumption** [cite reference:"vd2008visualizing"/]: t-SNE relies on the assumption of local linearity, which may not always hold true. This may lead to distortions in how global data structures are represented in the 2-D space. 
* **Limited clustering capabilities** [cite reference:"arora2018analysis"/]: t-SNE may only partially recover clusters under special circumstances. Visualized clusters should be interpreted cautiously, as they may not directly correspond to clusters in the original high-dimensional space.
* **Class-cluster assumption** [cite reference:"jeon2023classes"/]: One should not assume that classes in high-dimensional data will always manifest as distinct clusters in t-SNE's representation. The aglorithm highights local structures but may be misleading when interpreting global patterns.
        Given these caveats, it is important to use t-SNE cautiously in less well-studied use cases. 
        [br/]
        [br/]
        [button onClick:`showLimitation = false;`]
          Hide t-SNE's Limitations
        [/button]

      [/div]

    [/conditional]

    [div className:"text"]
      ### Visualizing the Rest of the Dataset
      To further visualize the global distribution of ResNet-34's embeddings on the entire CIFAR-10 dataset (with 10,000 images), 
      we also include a [span className:"emphasize"]hexagonal binning[/span] backdrop in our scatterplot. 
      This helps provide context even when only a subset of the dataset is actively displayed.
      [br/]
      [br/]
      The hexbin map shows the [span className:"emphasize"]global distribution[/span] of the entire dataset, 
      regardless of how many data points are visible in the foreground.  
      Each hexagon is colored according to the model's predicted class, 
      and the size of the hexagon represents the frequency of instances being predicted as that class in that region. 
      We use hexbins instead of circles here because using circles for all data points would lead to significant visual clutter, 
      making the scatterplot harder to interpret. 
      [br/]
      [br/]
      By employing hexagons, we can aggregate the data into manageable regions, reducing visual noise and providing a clearer view of the overall distribution.
      This way, the map shows the general trends in clustering based on model predictions, 
      allowing for quick identification of decision boundaries and similarly classified image groups.
    [/div]

    [div className:"text"]
      ### Exploring the Embedding Distribution
      [span className:"emphasize2"]
      With the help of the hexbin backdrop, take a look at the spatial distribution of ResNet-34's embeddings projected by t-SNE. What can you learn about ResNet-34's perception of the natural CIFAR-10 dataset?
      [br/]
      [/span]
    [/div]

    [var name:"showInsights" value:false /]

    [conditional if:`!showInsights`]
      [button onClick:`showInsights = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights`]

      [div className:"panel"]
        From the projection, we can see that the CIFAR-10 embeddings of ResNet-34 form small, distinct class clusters. When no attack has been conducted, most of the confusion seems to happen within the center of the projection, near the "boundaries" of the distinct clusters. Overall, ResNet-34 achieves high accuracy.
        [br/]
        [br/]
        However, something that may catch your attention is that, although a closer distance between instances generally means that the model perceives them as "more similar," we can see that there are classes that we don't deem similar at all close to each other, e.g., birds and automobiles.

        [button onClick:`showInsights = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]
  [/Step]

  [Step state:"slider"]

    [div className:"text"]
      ## CONDUCTING THE ATTACK
      ### Applying Perturbations
      Previously, we introduced the FGSM attack [cite reference:"goodfellow2014explaining"/], which generates [span className:"emphasize"]perturbations[/span] to craft adversarial examples. Perturbation refers to small, often imperceptible changes made to input data with the intent to mislead models into making incorrect predictions.
      [br/]
      [br/]
      Here, we utilize the FGSM attack with the [Equation]L^{\infty}[/Equation] norm to generate adversarial examples.
    [/div]

    [var name:"showLInfinity" value:false /]

    [conditional if:`!showLInfinity`]
      [button onClick:`showLInfinity = true;`]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:`showLInfinity`]

      [div className:"text"]
        ### FGSM with L-infinity
        Also known as the Chebyshev distance, the [Equation]L^{\infty}[/Equation] distance is commonly adapted by adversarial attacks to generate perturbed images by measuring the maximum pixel difference between two images. 
        For example, 
        if [Equation]\mathbf{x}[/Equation] is the original image input, 
        and [Equation]\mathbf{x}' = \mathbf{x} + \mathbf{n}[/Equation] is the adversarial output where [Equation]\mathbf{n}[/Equation] is equivalent to [Equation]\epsilon \cdot \text{sign}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y))[/Equation], 
        then the [Equation]L^{\infty}[/Equation] distance between [Equation]\mathbf{x}[/Equation] and [Equation]\mathbf{x}'[/Equation] is computed as the following:

        [Equation display:true]
        ||\mathbf{n}||_{\infty} = \max_{i} |n_i|.
        [/Equation]

        [div className:"panel"]
* [Equation]\mathbf{n}[/Equation]: This represents the perturbation added to the original image. Every element in [Equation]\mathbf{n}[/Equation] corresponds to the value change in a specific pixel. 
* [Equation]|n_i|[/Equation]: The absolute value of the change for pixel [Equation]i[/Equation], where [Equation]i[/Equation] indicates the pixel index.
* [Equation]\max_i |n_i|[/Equation]: This is the maximum absolute value of the perturbations applied to the image. It means looking at all the pixel changes and identify the one that is the largest.
* [Equation]||\mathbf{n}||_{\infty}[/Equation]: This is the notation for the [Equation]L^{\infty}[/Equation] norm.
        [/div]

        (Use the slider below to adjust [Equation]\epsilon[/Equation] and observe the changes in ResNet-34's embeddings.)

        [button onClick:`showLInfinity = false;`]
          Hide technical details
        [/button]

      [/div]

    [/conditional]

    [div className:"text"]
      [br/]
      Now, we will apply these perturbations to our data points to observe their effects.
    [/div]

    [div className:"slider_container" style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [div className:"slider_" style:`{margin: '0 auto'}`]
        None
        [Range value:perturbation_val min:0 max:0.03 step:0.01 /]
        0.03
        [br/]
        Perturbation Size (ε): [Display value:perturbation_val /]
      [/div]

    [/div]

    [div className:"text"]
      ### Accuracy vs. Robustness
      In AML, [span className:"emphasize"]accuracy (or natural accuracy)[/span] refers to the model's performance on clean data, while [span className:"emphasize"]robustness (or robust accuracy)[/span] measures its performance on adversarially perturbed data. 
      Check the bar chart below to see how accuracy on the entire dataset drops after applying perturbations.
    [/div]

    [BarComponent
       perturb:perturbation_val
    /]

    [div className:"text"]
      ### Exploring Dataset-level Attack Impact
      [span className:"emphasize2"]
      Adjust the slider and observe the changes in model embeddings and prediction accuracy. What interesting insights can you find?
      [/span]
    [/div]

    [var name:"showInsights2" value:false /]

    [conditional if:`!showInsights2`]
      [br/]
      [button onClick:`showInsights2 = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights2`]

      [div className:"panel"]
        When the perturbation size is set to values above 0, the data points within the scatterplot start to move rapidly, mostly towards the center of the plot. This suggests that the FGSM attack has drastically altered ResNet-34's perception of these images' features.
        [br/]
        [br/]
        As the perturbation size increases, ResNet-34's prediction accuracy continues to drop, eventually reaching 57% at a perturbation size of 0.03.
        [br/]
        [br/]
        [button onClick:`showInsights2 = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]

    [div className:"text"]
      ### Is It Imperceptible?
      To investigate specific images before and after the attack, [span className:"emphasize2"]adjust the perturbation slider above, then click on a data point in the embeddings[/span] and observe the natural and adversarial images below.
      [br/]
    [/div]
    [InstanceComponent
      perturb:perturbation_val
    /]
    [Table/]
    [div className:"text"]
      [br/]
      [span className:"emphasize2"]Can you spot the differences between the images?[/span]
    [/div]

    [var name:"showInsights3" value:false /]

    [conditional if:`!showInsights3`]
      [br/]
      [button onClick:`showInsights3 = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights3`]

      [div className:"panel"]
        The larger the perturbation size, the more obvious the perturbations are and the more effective the attack is. That's why attackers aim to find a middle point where the perturbations are subtle yet sufficient to lead to a misclassification. However, keep in mind that the CIFAR-10 dataset consists of 32 × 32 images, so the applied noise is relatively more apparent compared to high-resolution images (e.g., the panda image).
        [br/]
        [br/]
        [button onClick:`showInsights3 = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]

  [/Step]

  [Step state:"AT"]

    [div className:"text"]
      ## ADVERSARIAL TRAINING
      ### Training ResNet-34 Differently
      Now, let's take a look at a different ResNet-34. (Let's call it ResNet-34★.)
      [br/]
      [br/]
      On the right, we have loaded the CIFAR-10 embeddings of this new ResNet-34 when no attack has been conducted. This ResNet-34★ shares the same model architecture as the previous ResNet-34 we just explored, but it has been specifically trained with [span className:"emphasize"]adversarial training (AT)[/span] [cite reference:"madry2017towards"/].
    [/div]
    [div className:"text"]
      ### What is Adversarial Training (AT)?
      To counter adversarial attacks, various defense methods have been proposed to fortify model robustness against adversarial inputs. Adversarial training is currently the most effective defense, which trains classifiers with adversarial examples by adding them to the training set or through regularizations.
      [br/]
      [br/]
      Here, we use [span className:"emphasize"]TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES)[/span][cite reference:"zhang2019theoretically"/] to train ResNet-34★, one of the state-of-the-art adversarial training methods.

      [var name:"showTRADES" value:false /]

      [conditional if:`!showTRADES`]
        [button onClick:`showTRADES = true;`]
          Show more about TRADES
        [/button]
      [/conditional]

      [conditional if:`showTRADES`]
        ### What is TRADES?
        TRADES[cite reference:"zhang2019theoretically"/] is an advanced AT method designed to enhance the robustness of neural networks against adversarial attacks. 
        It focuses on balancing the trade-off between accuracy on unperturbed data and robustness against adversarial examples by introducing a [span className:"emphasize"]surrogate loss[/span] function. 
        This surrogate loss penalizes large deviations in predictions caused by small perturbations, ensuring that a model that can maintain relatively high natural accuracy while being resilient to adversarial manipulations. 
        [br/]
        [br/]
        Specifically, TRADES uses the following optimization problem for the loss function: 
        [Equation display:true]
          \min_{f} \mathbb{E} \left\{ \mathcal{L}(f(X), Y) + \beta \max_{X' \in \mathsf{Ball}(X, \epsilon)} \mathcal{L}(f(X), f(X')) \right\} 
        [/Equation]
        [div className:"panel"]
* [Equation]\min_f[/Equation] means we are finding the function [Equation]f[/Equation] that minimizes the overall loss.
* [Equation]\mathbb{E}\{...\}[/Equation] represents the *expected value*, meaning that we are minimizing the average loss over all inputs from the dataset.
* [Equation]L(f(X),Y)[/Equation] is the first part of the loss function; it represents the standard classification loss, which measures how well the model’s predictions on the clean data (i.e., [Equation]f(X)[/Equation]) match true labels [Equation]Y[/Equation]. 
* [Equation]\beta[/Equation] balances between the standard classification loss and the adversarial loss. A larger [Equation]\beta[/Equation] emphasizes on robustness, while a smaller [Equation]\beta[/Equation] prioritizes natural accuracy.
* [Equation]\max_{X' \in \text{Ball}(X, \epsilon)} L(f(X),f(X'))[/Equation] is the second part of the loss; it looks for an adversarial example [Equation]X'[/Equation] within a perturbation [Equation]\epsilon[/Equation] of [Equation]X[/Equation]. It measures how different the model's predictions are for the clean and adversarial examples. The goal is to ensure that the predictions for [Equation]X[/Equation] and [Equation]X'[/Equation] are as close as possible.
        [/div]
        To summarize:
1. The first part of TRADE's loss ensures the model performs well on clean data (i.e., [span className:"emphasize"]natural accuracy[/span]).
2. The second part prevents the model from changing its prediction too much when the input is perturbed by an attack (i.e., [span className:"emphasize"]adversarial robustness[/span]).
        [button onClick:`showTRADES = false;`]
          Show less about TRADES
        [/button]

      [/conditional]

    [/div]

    [div className:"text"]
      ### Applying Perturbations Again
      Now, let's try conducting the FGSM attack on ResNet-34★ and see what happens.
    [/div]

    [div className:"slider_container" style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [div className:"slider_" style:`{margin: '0 auto'}`]
        None
        [Range value:perturbation_val min:0 max:0.03 step:0.01 /]
        0.03
        [br/]
        Perturbation Size (ε): [Display value:perturbation_val /]
      [/div]

    [/div]

    [div className:"text"]
      [span className:"emphasize2"]
        What do you notice about ResNet-34★'s behavior compared to ResNet-34's?
        [/span]
    [/div]

    [var name:"showInsights4" value:false /]

    [conditional if:`!showInsights4`]
      [br/]
      [button onClick:`showInsights4 = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights4`]

      [div className:"panel"]
        Compared to the naturally trained ResNet-34, the FGSM attack barely changes how ResNet-34★ perceives the CIFAR-10 dataset (as the data points are barely moving). Hence, ResNet-34★ achieves higher robustness than ResNet-34. 
        [br/]
        [br/]
        [button onClick:`showInsights4 = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]

    [div className:"text"]
      ### How Come the Difference?
      Why does the FGSM attack drastically alter the features extracted by ResNet-34 from the CIFAR-10 dataset, but not those extracted by ResNet-34★?
      [br/]
      [br/]
      To understand this, let's closely examine the noises generated by FGSM for both ResNet-34 and ResNet-34★. Keep in mind that these noises are based on the [span className:"emphasize"]gradient of the loss function[/span], shedding light on the features each model relies on most for their predictions.
      [br/]
      [br/]
      [span className:"emphasize2"]Click on a data point and observe the perturbations generated by FGSM for ResNet-34 versus ResNet-34★.[/span]
    [/div]
    [br/]

    [ImgComponent
      perturb:perturbation_val
    /]

    [div className:"text"]
      [span className:"emphasize2"]What do you notice about the noises?[/span]
    [/div]

    [var name:"showInsights5" value:false /]

    [conditional if:`!showInsights5`]
      [br/]
      [button onClick:`showInsights5 = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights5`]

      [div className:"panel"]
        Compared to the noises generated for ResNet-34, the noises generated for ResNet-34★ have more defined shapes that resemble the original images. Since these noises are gradient-based, this suggests that ResNet-34★ relies on more human-interpretable features for classification, making it more robust.
        [br/]
        [br/]
        [button onClick:`showInsights5 = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]

  [/Step]

  [Step state:"summary"]
    [div className:"text"]
      ## SUMMARY & CONCLUSION

      In this guide, we explored adversarial attacks on CNNs, focusing on the FGSM attack and its impact on ResNet-34 models. We demonstrated how subtle perturbations can lead to significant misclassifications.

      ### What We Learned

      1. [span className:"emphasize"]Adversarial Attacks[/span]: Small, intentional changes to input data can deceive models into incorrect predictions. FGSM is a simple yet effective method to generate such adversarial examples.
      [br/]
      [br/]
      2. [span className:"emphasize"]Impact on Models[/span]: Applying FGSM showed a drastic drop in model accuracy with increasing perturbation size, highlighting the vulnerability of models to adversarial attacks.
      [br/]
      [br/]
      3. [span className:"emphasize"]Adversarial Training (AT)[/span]: TRADES, an advanced adversarial training method, balances accuracy and robustness, enhancing model resilience against adversarial perturbations.
      [br/]
      [br/]
      4. [span className:"emphasize"]Model Comparisons[/span]: ResNet-34★, trained with TRADES, demonstrated higher robustness to FGSM attacks compared to the naturally trained ResNet-34.
      [br/]
      [br/]
      Understanding adversarial attacks and defenses is important for developing robust AI systems. 
      By integrating advanced training techniques, 
      we can build models that are both accurate and robust to adversarial manipulations, 
      ensuring their reliability in real-world applications.

      ### Learn More About Adversarial Attacks

      If you are interested in exploring adversarial attacks and related topics further, here are some other online resources:
1. **TensorFlow Tutorial** - *Adversarial Attacks Using FGSM* ([link](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm))
2. **Pytorch Tutorial** - *Adversarial Example Generation* ([link](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html))
3. **SEI Blog (Carnegie Mellon University)** - *The Challenge of Adversarial Machine Learning* ([link](https://insights.sei.cmu.edu/blog/the-challenge-of-adversarial-machine-learning/))
4. **AdVis.js** - *Exploring Fast Gradient Sign Method* ([link](https://jlin.xyz/advis/))
5. **Bluff** - *Interactively Deciphering Adversarial Attacks on Deep Neural Networks* ([link](https://poloclub.github.io/bluff/))
    [/div]

  [/Step]

  [Step state:"references"]
    [References /]
  [/Step]

[/Scroller]
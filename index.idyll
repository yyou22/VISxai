[meta title:"Intro to Adversarial Attacks" description:"A Beginner's Introduction to Adversarial Attacks" /]

[var name:"scrollState" value:"loading" /]
[Fixed]
  [DRComponent
    state:scrollState
  /]
[/Fixed]

//scroller
[Scroller currentState:scrollState]

  [Graphic]
    [ul className:"circles__"]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
    [/ul]
    [div className:"background"]
    [/div]
  [/Graphic]

  [Step state:"abstract"]

    [CustomHeader
        title:"Panda or Gibbon? How CNNs Get Fooled by Input Noises"
        subtitle:"A Beginner's Introduction to Adversarial Attacks"
        date:"July 31, 2024"
        authors:`[
        { name: "Yuzhe You", link: "FIXME" },
        { name: "Jian Zhao", link: "FIXME" }
        ]` /]

    [div className:"line"][/div]

    [div className:"text"]
        ## ABSTRACT
        Though deep learning models have achieved remarkable success in diverse domains (e.g., faicial recognition, autonomous driving),
        these models have been proven to be quite brittle to perturbations around the input data. 
        [span className:"emphasize"]Adversarial machine learning (AML)[/span] studies attacks that can fool machine learning models into generating incorrect outcomes as well as the defenses against worst-case attacks to strength model robustness. 
        Specifically for image classification,it is challenging to understand [span className:"emphasize"]adversarial attacks[/span] due to their use of subtle perturbations that not human-interpretable, 
        as well as the variability of attack impacts influenced by attack methods, instance differences, or model architectures. 
        This guide will utilize interative visualizations to provide a non-expert introduction into adversarial attacks, 
        and provide comparisons of two popular attacks on different CNNs. 
    [/div]
  [/Step]

  [Step state:"beginning"]
    [div className:"text"]
      ## FROM THE VERY BEGINNING
      Before we use visualizations to understand the impacts of adversarial attacks on CNNs (yes, those are soon-to-be data points you see floating on the right), let's first go through some basics.
    [/div]
    [div className:"text"]
      ### What is an Adversarial Attack?
      In 2014, Goodfellow et al. showed that an adversarial image of a [span className:"emphasize"]panda[/span] could fool GoogLeNet into classifying it as [span className:"emphasize"]gibbon[/span] with high confidence, 
      leading to the birth of AML research. 
      An adversarial [span className:"emphasize"]"evasion" attack[/span] produces adversarial examples that are crafted with small, indistinguishable perturbations with the goal to result in model prediction errors, such as image misclassifications. 
    [/div]
    [figure]
      [img src:"static/images/tut1.png" className:"img_" /]
    [/figure]
    [div className:"text"]
      ### What is the FGSM Attack?
      The panda attack you just saw is called the [span className:"emphasize"]Fast Gradient Sign Method (FGSM)[/span] attack, 
      one of the first and most known adversarial attacks. 
      It is a [span className:"emphasize"]white-box attack[/span] (i.e., the attacker has access to model internals) that adjusts the input image by taking a step towards the sign of the backpropagated gradients. 
      The idea is that rather than working to minimize the loss like in model training, the FGSM attack adjusts the input data to maximize the loss. 
    [/div]
    [figure]
      [img src:"static/images/tut2.png" className:"img_" /]
    [/figure]
    [figure]
      [img src:"static/images/tut3.png" className:"img_2" / ]
    [/figure]
    [div className:"text"]
      ### How does the CNN see a gibbon then?
      While the attack logic is simple and intuitive, how come two panda images that appear indistinguishable to humans can seem so different to machine learning models? 
      Let's take a look at how the FGSM attack alters CNNs' perceptions of image datasets. 
    [/div]

  [/Step]

  [Step state:"dataset"]
    [div className:"text"]
      ## FGSM ON CIFAR-10
      ### CIFAR-10 Dataset
      On the right, we have loaded 100 sampled images from the [span className:"emphasize"]CIFAR-10 dataset[/span] in the scatterplot. 
      The CIFAR-10 dataset consists of 60,000 (32 Ã— 32) colored images from [span className:"emphasize"]10 different classes[/span] (50,000 training data and 10,000 testing data), 
      with 6,000 images per class. 
      Here, we randomly sampled 10 images from each class. 
      [br/]
      [br/]
      We use the following colors to represent these classes:
    [/div]
    [figure]
      [img src:"static/images/color legend.png" className:"img" / ]
    [/figure]
    [figure]
      [img src:"static/images/circle_.png" className:"img" / ]
    [/figure]
  [/Step]

[/Scroller]
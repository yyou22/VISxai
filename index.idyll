[meta title:"Intro to Adversarial Attacks" description:"A Beginner's Introduction to Adversarial Attacks" /]

[var name:"scrollState" value:"loading" /]
[Fixed]
  [DRComponent
    state:scrollState
  /]
[/Fixed]

//scroller
[Scroller currentState:scrollState]

  [Graphic][/Graphic]

  [Step state:"introduction"]

    [CustomHeader
        title:"Panda or Gibbon? How CNNs Get Fooled by Input Noises"
        subtitle:"A Beginner's Introduction to Adversarial Attacks"
        date:"July 31, 2024"
        authors:`[
        { name: "Yuzhe You", link: "FIXME" },
        { name: "Jian Zhao", link: "FIXME" }
        ]` /]

    [div className:"line"][/div]

    [div className:"text"]
        ## INTRODUCTION
        Though deep learning models have achieved remarkable success in diverse domains (e.g., faicial recognition, autonomous driving),
        these models have been proven to be quite brittle to perturbations around the input data. 
        [span className:"emphasize"]Adversarial machine learning (AML)[/span] studies attacks that can fool machine learning models into generating incorrect outcomes as well as the defenses against worst-case attacks to strength model robustness. 
        Specifically for image classification,it is challenging to understand [span className:"emphasize"]adversarial attacks[/span] due to their use of subtle perturbations that not human-interpretable, 
        as well as the variability of attack impacts influenced by attack methods, instance differences, or model architectures. 
        This guide will utilize interative visualizations to provide a non-expert introduction into adversarial attacks, 
        and provide comparisons of two popular attacks on different CNNs. 
    [/div]
  [/Step]

  [Step state:"beginning"]
    [div className:"text"]
      ## FROM THE VERY BEGINNING
      ### What is an Adversarial Attack?
      In 2014, Goodfellow et al. showed that an adversarial image of a [span className:"emphasize"]panda[/span] could fool GoogLeNet into classifying it as [span className:"emphasize"]gibbon[/span] with high confidence, 
      leading to the birth of AML research. 
      An adversarial [span className:"emphasize"]"evasion" attack[/span] produces adversarial examples that are crafted with small, indistinguishable perturbations with the goal to result in model prediction errors, such as image misclassifications. 
    [/div]
    [figure]
      [img src:"static/images/tut1.png" className:"img_" /]
    [/figure]
    ### What is the FGSM Attack?
  [/Step]

[/Scroller]
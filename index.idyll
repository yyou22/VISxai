[meta title:"Intro to Adversarial Attacks" description:"A Beginner's Introduction to Adversarial Attacks" /]

[var name:"perturbation_val" value:0 /]

[var name:"scrollState" value:"loading" /]
[Fixed]
  [DRComponent
    state:scrollState
    perturb:perturbation_val
  /]
[/Fixed]

//scroller
[Scroller currentState:scrollState]

  [Graphic]
    [ul className:"circles__"]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
      [li][/li]
    [/ul]
    [div className:"background"]
    [/div]
  [/Graphic]

  [Step state:"abstract"]

    [CustomHeader
        title:"Panda or Gibbon? How CNNs Get Fooled by Input Noises"
        subtitle:"A Beginner's Introduction to Adversarial Attacks"
        date:"July 31, 2024"
        authors:`[
        { name: "Yuzhe You", link: "FIXME" },
        { name: "Jian Zhao", link: "FIXME" }
        ]` /]

    [div className:"line"][/div]

    [div className:"text"]
        ## ABSTRACT
        Though deep learning models have achieved remarkable success in diverse domains (e.g., faicial recognition, autonomous driving),
        these models have been proven to be quite brittle to perturbations around the input data. 
        [span className:"emphasize"]Adversarial machine learning (AML)[/span] studies attacks that can fool machine learning models into generating incorrect outcomes as well as the defenses against worst-case attacks to strength model robustness. 
        Specifically for image classification,it is challenging to understand [span className:"emphasize"]adversarial attacks[/span] due to their use of subtle perturbations that not human-interpretable, 
        as well as the variability of attack impacts influenced by attack methods, instance differences, or model architectures. 
        This guide will utilize interative visualizations to provide a non-expert introduction into adversarial attacks, 
        and provide comparisons of two popular attacks on CNNs. 
    [/div]
  [/Step]

  [Step state:"beginning"]
    [div className:"text"]
      ## FROM THE BEGINNING
      Before we use visualizations to understand the impacts of adversarial attacks on CNNs (yes, those are soon-to-be data points you see floating on the right), let's first go through some basics.
    [/div]
    [div className:"text"]
      ### What is an Adversarial Attack?
      In 2014, Goodfellow et al. showed that an adversarial image of a [span className:"emphasize"]panda[/span] could fool GoogLeNet into classifying it as [span className:"emphasize"]gibbon[/span] with high confidence, 
      leading to the birth of AML research. 
      An adversarial [span className:"emphasize"]"evasion" attack[/span] produces adversarial examples that are crafted with small, indistinguishable perturbations with the goal to result in model prediction errors, such as image misclassifications. 
    [/div]
    [figure]
      [img src:"static/images/tut1.png" className:"img_" /]
    [/figure]
    [div className:"text"]
      ### What is the FGSM Attack?
      The panda attack you just saw is called the [span className:"emphasize"]Fast Gradient Sign Method (FGSM)[/span] attack, 
      one of the first and most known adversarial attacks. 
      It is a [span className:"emphasize"]white-box attack[/span] (i.e., the attacker has access to model internals) that adjusts the input image by taking a step towards the sign of the backpropagated gradients. 
    [/div]
      [figure]
      [img src:"static/images/tut2.png" className:"img_" /]
      [/figure]

    [var name:"showFGSM" value:false /]

    [conditional if:`!showFGSM`]
      [button onClick:`showFGSM = true;`]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:`showFGSM`]

      [div className:"text"]
        The idea is that rather than working to minimize the loss like in model training, the FGSM attack adjusts the input data to maximize the loss:
        [br/]
        [Equation display:true]
        \mathbf{x}' = \mathbf{x} +\epsilon \textrm{\textrm{sign}}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y)). 
        [/Equation]
        Here, it modifies image [Equation]\mathbf{x}[/Equation] by maximizing the loss [Equation]J(\theta, \mathbf{x}, y)[/Equation] towards the gradients' sign to produce the adversarial image [Equation]\mathbf{x}[/Equation], 
        where [Equation]y[/Equation] is the true label, [Equation]\theta[/Equation] is model parameters, and [Equation]\epsilon[/Equation] scales the perturbation. 
      [/div]
      [figure]
        [img src:"static/images/tut3.png" className:"img_2" / ]
      [/figure]

      [button onClick:`showFGSM = false;`]
        Hide technical details
      [/button]

    [/conditional]

    [div className:"text"]
        ### How does the CNN see a gibbon then?
        While the attack logic is simple and intuitive, how come two panda images that appear indistinguishable to humans can seem so different to machine learning models? 
        [br/]
        [br/]
        Let's take a look at how the FGSM attack alters CNNs' perceptions of image datasets. 
      [/div]

  [/Step]

  [Step state:"dataset"]
    [div className:"text"]
      ## DATASET & MODEL
      ### CIFAR-10 Dataset
      On the right, we have loaded 100 sampled images from the [span className:"emphasize"]CIFAR-10 dataset[/span] in the scatterplot. 
      The CIFAR-10 dataset consists of 60,000 (32 × 32) colored images from [span className:"emphasize"]10 different classes[/span] (50,000 training data and 10,000 testing data), 
      with 6,000 images per class. 
      Here, we randomly sampled 10 images from each class. 
      [br/]
      [br/]
      We use the following colors to represent these classes:
    [/div]
    [figure]
      [img src:"static/images/color legend.png" className:"img_" / ]
    [/figure]
    [div className:"text"]
      Each circle from the scatterplot represents an instance from the dataset, and is splitted into two halves: 
      the color of the left half represents its [span className:"emphasize"]ground truth label[/span], while the color of the right half represents the [span className:"emphasize"]model's prediction[/span] of the image. 
    [/div]
    [figure]
      [img src:"static/images/circle_.png" className:"img_3" / ]
    [/figure]
    [div className:"text"]
      ### ResNet-34 Model
      For this article, we will first use the [span className:"emphasize"]ResNet-34[/span] model and start by visualizing its perceptions of the CIFAR-10 datasdet by extracting its embeddings. 
      [br/]
      [var name:"showResNet" value:false /]

      [conditional if:`!showResNet`]
        [button onClick:`showResNet = true;`]
          Show more about ResNet-34
        [/button]
      [/conditional]

      [conditional if:`showResNet`]
        ResNet-34 is a [span className:"emphasize"]convolutional neural network (CNN)[/span] that is part of the Residual Networks family, 
        which utilizes [span className:"emphasize"]residual learning[/span]. 
        Residual learning helps the network to learn residual functions with reference to the layer inputs, 
        which addresses the vanishing gradient problem and allows the training of much deeper networks.
        This architecture consists of 34 layers, and it is widely adopted for its strong performance in image classification. 
        [br/]
        [br/]
        [button onClick:`showResNet = false;`]
          Show less about ResNet-34
        [/button]

      [/conditional]

      [br/]
      To extract ResNet-34's perception of the images, 
      we temporarily detach the final output layer to obtain the [span className:"emphasize"]embeddings[/span], which are high-dimensional representations capturing the essential features of the input images.
    [/div]
    [div className:"text"]
      ### Exploring the data points
      [span className:"emphasize2"]
      Start exploring the data points by hovering over them and observing their ground truth labels and ResNet-34's predictions of them.
      [/span]
    [/div]
  [/Step]

  [Step state:"data points"]
    [div className:"text"]
      ## INTERACTING WITH DATA POINTS
      ### Projecting onto a 2-D Space
      To reveal important patterns in the embeddings and transform them into a format that can be easily visualized, 
      we apply [span className:"emphasize"]dimensionality reduction[/span] to project the model embeddings into a lower dimension. 
      We start by using [span className:"emphasize"]t-SNE[/span] (t-distributed Stochastic Neighbor Embedding). 
      [br/]
      [br/]
      t-SNE works by converting similarities between data points into probabilities and then maps these points into a 2-D space while preserving the relative distances. 
      It highlights clusters and relationships in the data that aren't easily seen in higher dimensions.
      [br/]
      [br/]
      The resulting outputs are scaled to be used as the x- and y-coordinates of the instances in the scatterplot shown on the right. 
    [/div]
    [div className:"text"]
      ### Visualizing the Rest of the Dataset
      To further visualize the global distribution of ResNet-34's CIFAR-10 image embeddings, 
      we also include a [span className:"emphasize"]hexagonal binning[/span] backdrop in our scatterplot. 
      [br/]
      [br/]
      The hexbin map shows the [span className:"emphasize"]global distribution[/span] of the entire dataset, 
      even when only a subset of the dataset is displayed. 
      Each hexagon is colored according to the model's predicted class, 
      and the size of the hexagon represents the frequency of instances being predicted as that class in that region. 
      This way, the map shows the general trends in clustering based on model predictions, 
      allowing quick identification of decision boundaries and similarly classified image groups. 
    [/div]

    [div className:"text"]
      ### Exploring the Embedding Distribution
      [span className:"emphasize2"]
      With the help of the hexbin backdrop, take a look at the spatiality of ResNet-34's embeddings projected by t-SNE. 
      What can you learn about ResNet-34's perceptions of the natural CIFAR-10 dataset? 
      [br/]
      [/span]
    [/div]

    [var name:"showInsights" value:false /]

    [conditional if:`!showInsights`]
      [button onClick:`showInsights = true;`]
        Show insights
      [/button]
    [/conditional]

    [conditional if:`showInsights`]

      [div className:"panel"]
        From the projection, we can see that the CIFAR-10 embeddings of ResNet-34 form [span className:"emphasize"]small, distinct class clusters[/span].
        When no attack has been conducted,
        most of the confusion seems to happen within the center of the projection,
        near the "boundaries" of the distinct clusters.
        Overall, ResNet-34 achieves high accuracy. 
        [br/]
        [br/]
        However, something that may catch your attention is that, 
        although a closer distance between instances generally means that the model perceives them as "more similar," 
        we can see that [span className:"emphasize"]there are classes that we don't deem similar at all close to each other[/span], e.g., birds and automobiles.

        [button onClick:`showInsights = false;`]
          Hide insights
        [/button]

      [/div]

    [/conditional]
  [/Step]

  [Step state:"slider"]

    [div className:"text"]
      ## CONDUCTING THE ATTACK - FGSM
      ### Applying Perturbations
      Previously, we introduced the FGSM attack, which generate [span className:"emphasize"]perturbations[/span] to craft adversarial examples. 
      Perturbation refers to small, often imperceptible changes made to input data with the intent to mislead models into making incorrect predictions.  
      [br/]
      [br/]
      Here, we utilize FGSM attack with [Equation]L^{\infty}[/Equation] norm to generate adversarial examples. 
    [/div]

    [var name:"showLInfinity" value:false /]

    [conditional if:`!showLInfinity`]
      [button onClick:`showLInfinity = true;`]
        Show technical details
      [/button]
    [/conditional]

    [conditional if:`showLInfinity`]

      [div className:"text"]
        ### FGSM with L-infinity
        Also known as the Chebyshev distance, the [Equation]L^{\infty}[/Equation] distance is commonly adapted by adversarial attacks to generate perturbed images by measuring the maximum pixel difference between two images. 
        For example, 
        if [Equation]\mathbf{x}[/Equation] is the original image input, 
        and [Equation]\mathbf{x}' = \mathbf{x} + \mathbf{n}[/Equation] is the adversarial output where [Equation]\mathbf{n}[/Equation] is equivalent to [Equation]\epsilon \cdot \text{sign}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y))[/Equation], 
        then the [Equation]L^{\infty}[/Equation] distance between [Equation]\mathbf{x}[/Equation] and [Equation]\mathbf{x}'[/Equation] is computed as the following:

        [Equation display:true]
        ||\mathbf{n}||_{\infty} = \max_{i} |n_i|.
        [/Equation]

        Use the slider to adjust [Equation]\epsilon[/Equation] and observe the changes of ResNet-34's embeddings. 

        [button onClick:`showLInfinity = false;`]
          Hide technical details
        [/button]

      [/div]

    [/conditional]

    [div className:"text"]
      [br/]
      Now, we will apply these perturbations to our data points to observe their effects.
    [/div]

    [div className:"slider_container" style:`{display: 'flex', flexAlign: 'row', textAlign: 'center'}`]

      [div className:"slider_" style:`{margin: '0 auto'}`]
        None
        [Range value:perturbation_val min:0 max:0.03 step:0.01 /]
        0.03
        [br/]
        Perturbation Size (ε): [Display value:perturbation_val /]
      [/div]

    [/div]

  [/Step]

[/Scroller]
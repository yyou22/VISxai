[meta title:"Intro to Adversarial Attacks" description:"A Beginner's Introduction to Adversarial Attacks" /]

// Scroller Tutorial
[var name:"barStep" value:0 /]
[Scroller currentStep:barStep]

  [Graphic]
  [/Graphic]

  [Step]

    [CustomHeader
        title:"Panda or Gibbon? How CNNs Get Fooled by Input Noises"
        subtitle:"A Beginner's Introduction to Adversarial Attacks"
        date:"July 31, 2024"
        authors:`[
        { name: "Yuzhe You", link: "FIXME" },
        { name: "Jian Zhao", link: "FIXME" }
        ]` /]

    [div className:"line"][/div]

    [div className:"text"]
        ## INTRODUCTION
        Though deep learning models have achieved remarkable success in diverse domains (e.g., faicial recognition, autonomous driving),
        these models have been proven to be quite brittle to perturbations around the input data. 
        [span className:"emphasize"]Adversarial machine learning (AML)[/span] studies attacks that can fool machine learning models into generating incorrect outcomes as well as the defenses against worst-case attacks to strength model robustness. 
        Specifically for image classification,it is challenging to understand [span className:"emphasize"]adversarial attacks[/span] due to their use of subtle perturbations that not human-interpretable, 
        as well as the variability of attack impacts influenced by attack methods, instance differences, or model architectures. 
        This guide will utilize interative visualizations to provide a non-expert introduction into adversarial attacks, 
        and provide comparisons of two popular attacks on different CNNs. 
    [/div]
  [/Step]

  [Step]
    The **bar** on the right belongs to this first Scroller component. 
    
    Let's first look at how we can animate **within** a Scroller. Watch how it rotates 
    as you move to the next step.
  [/Step]

[/Scroller]